\section{Localization System}
\label{Localization System Methodology}
The basic idea behind our localization system implementation is to capture a frame at real time by a camera, then detect and decode the QR code in it. After that, we will be able to restore the QR code's global position from the RAM using the decoded data as described at \ref{Database connection from mobile}. Now, we only need to calculate the camera's position relative to the QR code, and then add the result together with the QR code's global position as follows:
\[ user\_global\_position = QR\_global\_position +  camera\_relative\_position\]
This is the pose estimation approach which is mentioned previously at section \ref{Pose Estimation with QR Codes BG}.

As we just mentioned, the QR code's global position is stored at a server so there is nothing to calculate here, but this is not the case of the camera's relative position. The process of calculating the later is somewhat complicated and not short, and there are several things that need to be calculated first, such as the QR code corners locations at the image frame, camera matrix, and the camera distortion coefficients. There are multiple libraries for localization purposes as we mentioned previously at \ref{localization libraries BG}, and we choose OpenCV due to its Android devices support, ease of use, simple API, and performance.

Everything related to the localization will be running at a separate thread from the main thread. This enables the System to perform other tasks simultaneously and independently.

\subsection{Camera Calibration}
We mentioned at \ref{Mobile Application Methodology Section} that users can chose the camera they want to use from the settings after connecting it to the phone. This approach rise the need of calibration system that users can use since the cameras' parameters are distinct and not known. Each camera need to be calibrated at least once at the first time. Then the camera parameters will be stored at the phone storage for reusing in the future. At first, users need to specify the number of rows and columns at the pattern and the width/height of each square in real world units. Then they need to take multiple photos to the pattern at different angles and distances and start the calibration process. After this point, everything else will be done automatically without the need of the users actions no more. 

The camera calibration system's implementation is the same as what was mentioned previously at section \ref{Camera Calibration Background}. First the program will iterate through each photo trying to estimate their inner corners points locations using the following OpenCV function:
\begin{lstlisting}[language=Java]
	boolean findChessboardCorners(
	Mat image,
	Size patternSize,
	MatOfPoint2f corners
	)
\end{lstlisting}

\subparagraph{Params:}

\begin{itemize}
	\item \textbf{image:}
	The current photo of the pattern.
	\item \textbf{patternSize:}
	Number of inner corners per a chessboard row and column.
	\item \textbf{corners:}
	Output array of detected corners.
\end{itemize}

After that, we used the OpenCV function cornerSubPix to refine the detected points. Finally, we used the following OpenCV function for calibration:
\begin{lstlisting}[language=Java]
	double calibrateCamera(
	List<Mat> objectPoints,
	List<Mat> imagePoints, 
	Size imageSize, 
	Mat cameraMatrix, 
	Mat distCoeffs, 
	List<Mat> rvecs, 
	List<Mat> tvecs
	)
\end{lstlisting}

\subparagraph{Params:}

\begin{itemize}
	\item \textbf{objectPoints:}
	List of real world inner points locations for each image.
	\item \textbf{imagePoints:}
	List of estimated inner points locations for each image.
	\item \textbf{imageSize:}
	Size of the calibrated pattern photo.
	\item \textbf{cameraMatrix:}
	Output matrix for the camera intrinsic values.
	\item \textbf{distCoeffs:}
	Output matrix for the camera distortion coefficients.
	\item \textbf{rvecs:}
	Output list contains the rotation of the calibration patterns for each image.
	\item \textbf{tvecs:}
	Output list contains the translation of the calibration patterns for each image.
\end{itemize}

\subsection{Camera's Relative Pose}
The first step is to capture frames using a camera at real time. We used cameraX to do so since it is the best for Android devices as we mentioned at \ref{localization libraries BG}. Then, each captured frame will be passed to a function for QR code detecting, decoding, and pose estimation, so finally we can calculate camera's relative position. For QR code detecting, there are bunch of useful libraries we can for QR code detecting and decoding as we mentioned previously at \ref{qr decode libraries}. We choose Google's ML kit due to its simplicity and good performance. After successfully detecting the QR code, Google's ML kit will calculate the QR code corners locations for us.

Finally, we use OpenCV's solvePnP to calculate the QR pose function as follow:

\begin{lstlisting}[language=Java]
	boolean solvePnP(
	MatOfPoint3f objectPoints, 
	MatOfPoint2f imagePoints, 
	Mat cameraMatrix, 
	MatOfDouble distCoeffs, 
	Mat rvec, 
	Mat tvec
	)
\end{lstlisting}

\subparagraph{Params:}

\begin{itemize}
	\item \textbf{objectPoints:}
	The QR code's four corners locations relative to its center in real world units, such as cm, mm, etc.
	\item \textbf{imagePoints:}
	The locations of the QR code's corners inside the captured frame in pixels.
	
	\item \textbf{cameraMatrix \& distCoeffs:}
	These two are crucial for calculating the QR code's pose since they represent the intrinsic values of a camera and its distortion coefficients.
	
	\item \textbf{rvec \& tvec:}
	These are output vectors for the code's rotation(rvec) and translation(tvec) relative to the camera.
\end{itemize}
imagePoints is calculated after successfully detecting the QR code by Google's ML kit.

Now we got the QR code's translation vector relative to the camera, we just multiply it with -1 to get the camera translation relative to the QR code.